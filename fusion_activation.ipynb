{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from fastai.text.all import *\n",
    "from fastai.vision.all import *\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils import get_dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "# python RNG\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "# pytorch RNGs\n",
    "import torch\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# numpy RNG\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "# # tensorflow RNG\n",
    "# tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIM=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetActs(Transform):\n",
    "    def encodes(self, x):        \n",
    "        img_file = text_file = None\n",
    "        \n",
    "        if x[\"has_text\"]:\n",
    "            text_file = Path(x[\"activation_path\"] + \".npy\")\n",
    "            if x[\"has_image\"]:\n",
    "                img_file = Path(text_file.as_posix().replace(\"text\", \"img\").replace(\"npy\", \"pt\"))\n",
    "        else:\n",
    "            img_file = Path(x[\"activation_path\"] + \".pt\")\n",
    "        \n",
    "        if img_file is None:\n",
    "            img_act = torch.zeros((4096))\n",
    "        else:\n",
    "            img_act = torch.load(img_file)\n",
    "                            \n",
    "        if text_file is None:\n",
    "            text_act = torch.zeros((3840))\n",
    "            text_none = True\n",
    "        else:\n",
    "            text_act = tensor(np.load(text_file))\n",
    "        \n",
    "        img_none = img_file == None\n",
    "        text_none = text_file == None\n",
    "                            \n",
    "        return (img_act, text_act, img_none, text_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgTextFusion(Module):\n",
    "    def __init__(self, head, embs_for_none=True, img_emb_dim=4096, text_emb_dim=3840):\n",
    "        self.head = head.cuda()\n",
    "        self.embs_for_none = embs_for_none\n",
    "        if embs_for_none:\n",
    "            self.img_none_emb = torch.nn.Embedding(num_embeddings=1, embedding_dim=img_emb_dim).cuda()\n",
    "            self.text_none_emb = torch.nn.Embedding(num_embeddings=1, embedding_dim=text_emb_dim).cuda()\n",
    "            self.index= tensor(0).cuda()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        img_act, text_act, img_none, text_none = x\n",
    "        if self.embs_for_none:\n",
    "            img_act[img_none] = self.img_none_emb(self.index)\n",
    "            text_act[text_none] = self.text_none_emb(self.index)\n",
    "        return self.head(torch.cat([img_act, text_act], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_head(nf, n_out, lin_ftrs=None, ps=0.5, bn_final=False, lin_first=False):\n",
    "    \"Model head that takes `nf` features, runs through `lin_ftrs`, and out `n_out` classes.\"\n",
    "    lin_ftrs = [nf, 512, n_out] if lin_ftrs is None else [nf] + lin_ftrs + [n_out]\n",
    "    ps = L(ps)\n",
    "    if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n",
    "    actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n",
    "    layers = []\n",
    "    if lin_first: layers.append(nn.Dropout(ps.pop(0)))\n",
    "    for ni,no,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], ps, actns):\n",
    "        layers += LinBnDrop(ni, no, bn=True, p=p, act=actn, lin_first=lin_first)\n",
    "    if lin_first: layers.append(nn.Linear(lin_ftrs[-2], n_out))\n",
    "    if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = torch.load(\"./data/fusion_dl_v2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.train.shuffle = False; dls.train.get_idxs()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.train.drop_last = False; dls.train.drop_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = torch.load(\"./data/test_dl_fusion_text.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = create_head(4096 + 3840, OUT_DIM, lin_ftrs=[128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImgTextFusion(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x7fb551f80dc0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.load(\"best_fusion_128_moreEpochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetActs(HookCallback):\n",
    "    def __init__(self, modules=None, remove_end=True, detach=True, cpu=True):\n",
    "        super().__init__(modules, None, remove_end, True, detach, cpu)\n",
    "        self.acts = L()\n",
    "    def hook(self, m, i, o): return o\n",
    "    def after_pred(self): self.acts += self.hooks.stored\n",
    "    def before_fit(self):\n",
    "        super().before_fit()\n",
    "        self.acts = L()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x7fb551f80dc0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.add_cb(GetActs([learn.model.head[3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(#1) [0.34672975540161133]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acts = learn.get_acts.acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([107577, 128])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_acts = torch.cat(list(valid_acts)); valid_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(TensorImage([[1.0691e-05, 3.0703e-04, 1.8721e-02, 9.8094e-01, 1.1829e-05, 9.8704e-06],\n",
       "         [2.2204e-08, 1.0903e-06, 6.2452e-08, 9.9998e-01, 2.2554e-05, 2.6479e-07],\n",
       "         [1.0612e-08, 2.4040e-06, 2.9739e-08, 9.9992e-01, 7.7347e-05, 2.0426e-07],\n",
       "         ...,\n",
       "         [5.4627e-04, 1.5725e-01, 6.8867e-04, 6.6404e-01, 1.7646e-01, 1.0221e-03],\n",
       "         [5.9546e-05, 1.2693e-02, 4.5226e-05, 6.1158e-01, 3.7543e-01, 1.8918e-04],\n",
       "         [6.4312e-04, 3.2100e-02, 2.9593e-04, 8.6563e-01, 1.0007e-01, 1.2532e-03]]),\n",
       " TensorCategory([3, 3, 3,  ..., 4, 4, 4]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.get_preds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acts = learn.get_acts.acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([162187, 128])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acts = torch.cat(list(train_acts)); train_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_acts) == len(dls.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(TensorImage([[2.8076e-11, 8.1904e-08, 1.8519e-11, 1.0000e+00, 2.9645e-07, 6.3525e-10],\n",
       "         [1.7164e-11, 3.6945e-08, 2.5709e-11, 1.0000e+00, 1.1012e-06, 1.4324e-10],\n",
       "         [1.1687e-08, 1.5438e-03, 3.3145e-07, 9.9845e-01, 7.9952e-06, 4.3045e-09],\n",
       "         ...,\n",
       "         [9.1729e-12, 2.2881e-07, 4.8685e-11, 1.0000e+00, 6.5150e-07, 1.8891e-10],\n",
       "         [2.4747e-11, 3.0487e-08, 7.3309e-12, 1.0000e+00, 1.5077e-07, 9.6348e-10],\n",
       "         [2.4747e-11, 3.0487e-08, 7.3309e-12, 1.0000e+00, 1.5077e-07, 9.6348e-10]]),\n",
       " TensorCategory([3, 3, 3,  ..., 3, 3, 3]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.get_preds(dl=test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acts = learn.get_acts.acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([95526, 128])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acts = torch.cat(list(test_acts)); test_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(test_acts) == len(test_dl.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = tensor(dls.train.items[dls.train.items[\"has_text\"]].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acts_filtered = torch.index_select(train_acts, 0, train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx = tensor(dls.valid.items[dls.valid.items[\"has_text\"]].index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_acts_filtered = torch.index_select(valid_acts, 0, valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([149217, 128]), torch.Size([94735, 128]), torch.Size([95526, 128]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acts_filtered.shape, valid_acts_filtered.shape, test_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving example 94735\r"
     ]
    }
   ],
   "source": [
    "for idx, item in dls.valid.items[dls.valid.items[\"has_text\"]].iterrows():\n",
    "    filename = Path(item[\"activation_path\"].replace(\"text\", \"fusion\"))\n",
    "    filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(valid_acts[idx].clone(), filename.as_posix() + \".pt\")\n",
    "    print(f\"Saving example {idx+1}\", end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving example 149217\r"
     ]
    }
   ],
   "source": [
    "for idx, item in dls.train.items[dls.train.items[\"has_text\"]].iterrows():\n",
    "    filename = Path(item[\"activation_path\"].replace(\"text\", \"fusion\"))\n",
    "    filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(train_acts[idx].clone(), filename.as_posix() + \".pt\")\n",
    "    print(f\"Saving example {idx+1}\", end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving example 95526\r"
     ]
    }
   ],
   "source": [
    "for idx, item in test_dl.items[test_dl.items[\"has_text\"]].iterrows():\n",
    "    filename = Path(item[\"activation_path\"].replace(\"text\", \"fusion\"))\n",
    "    filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(test_acts[idx].clone(), filename.as_posix() + \".pt\")\n",
    "    print(f\"Saving example {idx+1}\", end='\\r', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai2",
   "language": "python",
   "name": "fastai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
